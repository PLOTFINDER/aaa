{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook for the AAA course\n",
    "## **Authors**: BRETECHE Youenn & YAKOUBOV Anas\n",
    "## Presentation\n",
    "This notebook is a part of the AAA course. The goal is to predict the price of houses in Melbourne using a dataset from Kaggle. We will use a Linear Regression model to predict the price of the houses. Thanks to the dataset, we have several features that can be used to predict the price of the houses. We will use these features to train the model and evaluate it.\n",
    "\n",
    "This notebook is divided into several parts in order to follow the steps needed before training the model.\n",
    "\n",
    "For the moment, this notebook is only cleaning the data and trying to apply a Linear Regression model to the data. The next steps will be to try different models and compare them to see which one is the best for this dataset.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "housing = pd.read_csv(\"dataset.csv\")\n",
    "target_name = \"Price\"\n",
    "data = housing.drop(columns=target_name)\n",
    "target = housing[target_name]\n",
    "\n",
    "print(f\"Dataset size: {data.shape}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Columns description\n",
    "**Rooms**: Number of rooms\n",
    "\n",
    "**Price**: Price in dollars\n",
    "\n",
    "**Method**: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available.\n",
    "\n",
    "**Type**: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.\n",
    "\n",
    "**SellerG**: Real Estate Agent\n",
    "\n",
    "**Date**: Date sold\n",
    "\n",
    "**Distance**: Distance from CBD\n",
    "\n",
    "**Regionname**: General Region (West, North West, North, North east â€¦etc)\n",
    "\n",
    "**Propertycount**: Number of properties that exist in the suburb.\n",
    "\n",
    "**Bedroom2**: Scraped # of Bedrooms (from different source)\n",
    "\n",
    "**Bathroom**: Number of Bathrooms\n",
    "\n",
    "**Car**: Number of carspots\n",
    "\n",
    "**Landsize**: Land Size\n",
    "\n",
    "**BuildingArea**: Building Size\n",
    "\n",
    "**CouncilArea**: Governing council for the area"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "numerical_features = [\"Rooms\", \"Distance\", \"Propertycount\", \"Bedroom2\", \"Bathroom\", \"Car\", \"Landsize\", \"BuildingArea\"]\n",
    "\n",
    "categorical_features = [\"Type\", \"SellerG\", \"Regionname\", \"CouncilArea\"]\n",
    "\n",
    "data = data[numerical_features + categorical_features]\n",
    "data.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data preprocessing\n",
    "Building area has missing values, we check the percentage of missing values"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data[\"BuildingArea\"].isna().mean() * 100"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that 47% of the values are missing, if we drop the rows with missing values we will lose a lot of data\n",
    "So we will remove the column"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = data.drop(columns=[\"BuildingArea\"])\n",
    "numerical_features.remove(\"BuildingArea\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets visualize the data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.hist(bins=50, figsize=(20, 15))"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's visualize the correlation matrix using a heatmap"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "data_with_target = data.copy()\n",
    "data_with_target[target_name] = target\n",
    "\n",
    "sns.heatmap(data_with_target[numerical_features + [target_name]].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that Rooms and Bedroom2 are highly correlated. But also that Propertycount and Distance are negatively correlated with Price, so they might be good predictors. We can also see that Price is highly correlated with Rooms and Bedroom2 which makes sense since the more rooms a house has the more expensive it is.\n",
    "\n",
    "\n",
    "Let's also visualize the scatter matrix of the numerical features.\n",
    "It will help us see the distribution of the data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scatter_matrix(data[numerical_features], figsize=(20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see the same results as before, Rooms and Bedroom2 are highly correlated, and Propertycount and Distance are negatively correlated with Price. We can also see that Landsize has a long tail distribution, which means that there are some outliers in the data.\n",
    "\n",
    "\n",
    "Landsize seems to have a long tail distribution"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data[[\"Landsize\"]].describe()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If we remove the outliers we can see the distribution more clearly, but we will lose data, so we will keep the outliers.\n",
    "\n",
    "We check all the columns with missing values"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.isna().mean() * 100"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that Car column has missing values, we can use the most frequent value to fill the missing values"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data[\"Car\"].value_counts()\n",
    "\n",
    "data[\"Car\"] = data[\"Car\"].fillna(data[\"Car\"].mode()[0])\n",
    "\n",
    "data.isna().mean() * 100"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "Now that our data are cleaned, we can start applying transformations to them and train the model.\n",
    "This will be done later in the following steps of the notebook. However, for now, they are just a draft and this version is only focused on cleaning the data. You can take a look but don't expect much from it."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## [DRAFT] Applying transformations, training the model and evaluating it"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pipeline creation\n",
    "We create a pipeline to apply the transformations to the data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "numerical_features_transformer = Pipeline(steps=[\n",
    "    # (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "], verbose=True)\n",
    "\n",
    "categorical_features_transformer = Pipeline(steps=[\n",
    "    # (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    # (\"encoder\", OneHotEncoder())\n",
    "    (\"encoder\", OrdinalEncoder())\n",
    "], verbose=True)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_features_transformer, numerical_features),\n",
    "        (\"cat\", categorical_features_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# We also need to define the classifier, which in this case is a Linear Regression model\n",
    "# classifier = LinearRegression()\n",
    "# classifier = LogisticRegression()\n",
    "classifier = DecisionTreeRegressor()\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", classifier)\n",
    "])\n",
    "\n",
    "# Now we have a pipeline that can apply the transformations to the data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training and testing sets\n",
    "Splitting the data into training and testing sets"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.shape"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "target.shape"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_size = 0.2 # 20% of the data will be used for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=test_size)\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")\n",
    "print(f\"Training target size: {y_train.shape}\")\n",
    "print(f\"Testing target size: {y_test.shape}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Applying transformations\n",
    "Applying the transformations to the training and testing sets"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.fit(X_train, y_train)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating the model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train.head()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_train.head()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = [\n",
    "#     {\"preprocessor__num__imputer__strategy\": [\"mean\"]},\n",
    "#     # {\"preprocessor__cat__imputer__strategy\": [\"most_frequent\"]}\n",
    "# ]\n",
    "#\n",
    "# cv = GridSearchCV(model, param_grid, cv=5, scoring=\"neg_mean_squared_error\",\n",
    "#                            verbose=2, n_jobs=8)\n",
    "#\n",
    "# cv.fit(X_train, y_train)\n",
    "\n",
    "cv = cross_validate(model, X_train, y_train, cv=5, return_estimator=True, n_jobs=5, verbose=2)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We can see the best parameters\n",
    "# cv.best_params_\n",
    "cv\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's evaluate the model on the test set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Transform the test set using the pipeline\n",
    "# X_test_transformed = model.named_steps[\"preprocessor\"].transform(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(y_test.head())"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(model.predict(X_test.head()))"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
